{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab46773a-66e9-420a-9811-be81da1af05d",
   "metadata": {},
   "source": [
    "## Get (Relevant) Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04143dc3-f6e0-4a21-9e70-ca2343a10189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U \"pyDataverse==0.2.1\"\n",
    "import os\n",
    "import requests\n",
    "from pyDataverse.api import Api\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6e2def-fd46-466f-8b56-5a166cf7df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token.txt', 'r') as f:\n",
    "    token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e634758f-fffb-4ce6-a1d8-a0a646ef324f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = Api('https://dataverse.harvard.edu/', token)\n",
    "api.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da42828d-d429-424d-add4-65516c25b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files_info(file_name, df):\n",
    "    files = []\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        doi = r.persistentUrl.replace('https://doi.org/', 'doi:')\n",
    "        #print(doi)\n",
    "        dataset = api.get_dataset(doi)\n",
    "        if dataset.status_code == 200:\n",
    "            j = dataset.json()\n",
    "            if 'latestVersion' in j['data']:\n",
    "                for file in j['data']['latestVersion']['files']:\n",
    "                    fid = file['dataFile']['id']\n",
    "                    fn = file['dataFile']['filename']\n",
    "                    if fn.endswith('.R'):\n",
    "                        #print(fid, fn)\n",
    "                        files.append({'doi': doi, 'fid': fid, 'fn': fn})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    files_df = pd.DataFrame(files)\n",
    "    \n",
    "    if not os.path.exists('files_dfs'):\n",
    "        os.makedirs('files_dfs')\n",
    "        \n",
    "    # Write the DataFrame to a CSV file\n",
    "    files_df.to_csv(f'files_dfs/{file_name}_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b94a832-4b40-4835-8da5-17095aa5aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_path):\n",
    "    # Get the filename without the path and extension\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset = ['persistentUrl'])\n",
    "\n",
    "    return file_name, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faec45da-a85a-4853-a2f1-29ed5e071beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 652/652 [08:26<00:00,  1.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:52<00:00,  1.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 828/828 [08:56<00:00,  1.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [06:04<00:00,  1.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [04:59<00:00,  1.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 525/525 [06:26<00:00,  1.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 336/336 [03:19<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all files in the \"datasets\" folder\n",
    "datasets_folder = 'datasets'\n",
    "all_files = os.listdir(datasets_folder)\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(datasets_folder, file)\n",
    "        outs = read_csv_file(file_path)\n",
    "        extract_files_info(outs[0], outs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de515a99-76ad-463d-9a82-5b12a40b420d",
   "metadata": {},
   "source": [
    "### Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1229c3a-cc68-4ea2-b0aa-a97e1e970b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, fn):\n",
    "    local_filename = fn # url.split('/')[-1]\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "        return local_filename\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download file: {url}, Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6818dd3-e287-4739-9723-e2b52e83fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_wrapper(args):\n",
    "    fid, fn, doi, token, file_name = args\n",
    "    url = 'https://dataverse.harvard.edu//api/v1/access/datafile/%s?key=%s' % (fid, token)\n",
    "    path = os.path.join('scripts', file_name, doi.split('/')[-1])\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "        pass  # Skip creating the directory if it already exists\n",
    "    \n",
    "    lfn = os.path.join(path, fn)\n",
    "    if not os.path.exists(lfn):\n",
    "        download_file(url, lfn)\n",
    "        \n",
    "def download_files(file_path, num_workers=4):\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_list = [(f['fid'], f['fn'], f['doi'], token, file_name) for _, f in df.iterrows()]\n",
    "    \n",
    "    with Pool(num_workers) as pool:\n",
    "        list(tqdm(pool.imap(download_file_wrapper, file_list), total=len(file_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c750fdbd-f7f6-43c4-b4ad-3c78de513660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1276/1276 [00:00<00:00, 9230.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1948/1948 [00:00<00:00, 14474.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1132/1132 [00:00<00:00, 14443.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 175/175 [00:00<00:00, 32728.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1959/1959 [00:00<00:00, 14205.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1989/1989 [00:00<00:00, 13548.29it/s]\n",
      "  0%|                                                                                                                                | 0/577 [00:00<?, ?it/s]ERROR:root:Failed to download file: https://dataverse.harvard.edu//api/v1/access/datafile/3336522?key=13a0bf9d-2b1c-4d00-b410-9fe66e1f3c74, Error: 404 Client Error: Not Found for url: https://dataverse.harvard.edu//api/v1/access/datafile/3336522?key=13a0bf9d-2b1c-4d00-b410-9fe66e1f3c74\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 577/577 [01:24<00:00,  6.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all files in the \"datasets\" folder\n",
    "dataset_files_folder = 'files_dfs'\n",
    "all_files = os.listdir(dataset_files_folder)\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(dataset_files_folder, file)\n",
    "        download_files(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88f2ce-b612-42b1-97c0-b799b4db0ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
